{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 2 | Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Course:** CSE291  \n",
    "**Due:**  Monday, May 15, 11:55pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Random Forests\n",
    "\n",
    "---\n",
    "In this problem set you will be implementing **Random Forest** (RF) classifiers and comparing their performance with different tree sizes and number of trees. RFs were discussed in the lectures and the algorithm can be found in the slides. In the original formulation each of classification tree is grown to its full-extent. You are going to implement a variant where every tree can be grown to maximum depth of five. This will be in addition to implementing the original. You *can* use a library such as sklearn for only just the decision tree, you must write the code to bootstrap, fit and query the ensemble.\n",
    "\n",
    "For each of **MNIST** and **Covertype** datasets, you are required to do the following:\n",
    "\n",
    "1. Split the data into 75% training and 25% test sets.\n",
    "2. Train two types of RFs: \n",
    "  * one in which each of the decision trees is grown to the full extent, \n",
    "  * and the other in which the maximum tree depth is five.\n",
    "3. Do this for number of trees (B) from 1 to a reasonable number (minimum one hundred). You should choose a reasonable number by investigating the B at which the errors reach a plateau or starts increasing. [15 points]\n",
    "4. Using the trained forests, find and visualize a sorting of the most important features (i.e. Gini importance). [15 points]\n",
    "5. Show the final confusion matrix for both types of RF described above. [5 points]\n",
    "6. For each of the two types plot the training error, test error, and [out-of-bag error] w.r.t. the number of trees. [5 points]\n",
    "7. Comment on the trends you observed with increasing B for the two types of RFs. [5 points]\n",
    "8. Analyze and explain the trends you noted such as what features the forest found most important. [5 points]\n",
    "\n",
    "You might like to train the RF with the maximum number of trees first and then sample from it to simulate RFs with less trees. We did not discuss the parameters m (the number of randomly chosen dimensions at each split) here, nor the size of each of the bootstrap sets. You are expected to make your own choice and reason for them.\n",
    "\n",
    "> **Out-Of-Bag Error:** Let the training set be $T$ and the bootstrap training sets $T_k$ for the $k^{th}$ classifier. For each data point $(x, y)$ in $T$ aggregate over the votes of only those classifiers that were not trained on $(x, y)$. This is the out-of-bag classifier. The out-of-bag error is then the error of this classifier averaged over the training set.\n",
    "\n",
    "[out-of-bag error]: https://en.wikipedia.org/wiki/Out-of-bag_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "---\n",
    "Another committee method is Boosting where each member is trained on the same training set. However, the data points are weighed differently and each of them is only a weak learner. For this part you will implement *a* multiclass variant of AdaBoost, such as **AdaBoost.M2**,**Sequential AdaBoost**, etc. The algorithm for these are given in the slides and the papers we have discussed. We will again compare two different types of committee members.\n",
    "\n",
    "For each of **MNIST** and **Covertype** datasets, you are required to do the following:\n",
    "\n",
    "1. Split the dataset into 75% and 25% of training and test sets.\n",
    "2. Implement AdaBoost with two types of weak learners:\n",
    "  * one in which the learner is a decision stump (one level tree)\n",
    "  * and the other one in which the maximum tree depth is ten.  \n",
    "3. Do this for a reasonable number of iterations. You can choose the reasonable number by investigating when the test errors start increasing. [30 points]\n",
    "4. Show the final confusion matrix for both types of boosting described above. [5 points]\n",
    "5. Analyze and explain the trends you noted that may justify your results and parameter choices. [5 points]\n",
    "6. For each of the two types plot the training error, test error, w.r.t. the number of iterations. [5 points]\n",
    "7. Comment on the trends you observed with increasing iterations for the two types of learners. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "---\n",
    "Note that you may want try your classifier on smaller sets, such as the previous wine dataset, while developing to reduce training time between debugging, or provide sanity checks in prediction accuracy against naive bayes.\n",
    "\n",
    "| Dataset | Description | Data |\n",
    "|---|---|---|\n",
    "| MNIST | [[Description]](https://www.kaggle.com/c/digit-recognizer) | [[Data]](https://www.kaggle.com/c/digit-recognizer/data) |\n",
    "| Covertype | [[Description]](https://archive.ics.uci.edu/ml/datasets/Covertype) | [[Data]](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Submit\n",
    "\n",
    "---\n",
    "Submit your PDF report to Gradescope, and then same PDF with code in a zip file to Tritoned, as Gradescope does not take code submission. You can use any programming language, but we will prefer `Python` or `Matlab`. The code should be easily runnable. Please include a README.md file that describes how the TAs are to run your code. Note, you do not need to include the original data sets in the zip file (the is also a Blackboard upload limit prevents this), however please make it easy for the TAs to run you code, such that pasting the extracted course dataset folders into the root project zip folder would work, e.g something like this:\n",
    "\n",
    "```\n",
    "$ unzip Doe-John-HW2.zip\n",
    "$ unzip wine.zip -d Doe-John-HW2/wine\n",
    "$ unzip MNIST.zip -d Doe-John-HW2/MNIST \n",
    "$ tree Doe-John-HW2\n",
    "Doe-John-HW2\n",
    "|-- HW2.ipynb\n",
    "|-- HW2.pdf\n",
    "|-- MNIST\n",
    "|   |-- test.csv\n",
    "|   |-- train.csv\n",
    "|-- my_code\n",
    "|   |-- foo.py\n",
    "|   |-- bar\n",
    "|   |-- ...\n",
    "|-- README\n",
    "|-- wine\n",
    "    |-- wine.data\n",
    "\n",
    "6 directories, 7 files\n",
    "$ cat Doe-John-HW2/README\n",
    "To run my code, open HW2.ipynb and ...\n",
    "```\n",
    "\n",
    "Additionally:\n",
    "\n",
    "* You must submit your own code to get credit. No code submitted means a zero in the assignment.\n",
    "* If you are using a library from somewhere else, please mention it here as well. We hope you use Piazza for this so more people benefit.\n",
    "  * Again, do not simply use a library functions that already implement boosting or random forests for you, although you may build from an existing decision tree structure implementation.\n",
    "* You are encouraged to discuss and help others with anything short of giving them your code. There are many references on-line, especially with MNIST. However, you MAY NOT use code from the Internet.\n",
    "  * If you are working with a partner please make sure that **both** of your names are on the final PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Suggestions\n",
    "\n",
    "---\n",
    "While examining the results from the two different types of random forests, you may observe the version with unlimited depth performing poorly in test error. Try reasoning why and generate some figures to backup your hypothesis. Also, while examining error rates over the number of estimators in a Boosted ensemble, you notice an effect of diminishing returns or a plateauing of performance. Again, try derive hypothesize and show why it would be the case.\n",
    "\n",
    "Also, references to extra resources to support your feature importance observations (specially for Covertype dataset) are appreciated but not required."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 3 | KNN & SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Course:** CSE291  \n",
    "**Due:**  Tuesday, June 6th, 11:55pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN\n",
    "\n",
    "---\n",
    "In the lectures we discussed k-Nearest Neighbor based classification. In this assignment, we are going to use them. We discuss the methods briefly in the following, but refer to slides and textbook for more details on these technique.\n",
    "\n",
    "Using the notations from the class slide, for K-NN classifier we have:\n",
    "\n",
    "$$ \\hat{f}_{kNN}(x) = \\underset{y}{\\operatorname{argmax}}\\hat{p}_{kNN}(x|y)\\hat{P}(y) = \\underset{y}{\\operatorname{argmax}}\\frac{k_y}{n_y \\Delta_{k,x}}\\frac{n_y}{N} = \\underset{y}{\\operatorname{argmax}}k_y $$\n",
    "\n",
    "Distance metric $d(X^i,X^j) : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ between data points ${X} \\in \\mathbb{R}^n$ defines a measure of *similarity*. So distance between data-points belonging to same cluster should be lower than that between points belonging to separate clusters. Distance metric plays an important role for successful classification or density estimation. And its true for K-NN also. You are encouraged to try different distance metric for your K-NN classifier apart from the usual Euclidean distance metric $d(X^i,X^j) = \\|X^i-X^j\\| = \\sqrt{(X^i-X^j)^T(X^i-X^j)}$. You are required to scale the different dimensions of the data dfferently. Whitening of data can be a starting point for doing the scaling, but you can imagine that for determining neighbors certain dimensions are more important and thus should be weighted more. Another method [1, 2] is to learn the distance metric from the data itself. Usually we choose a fixed type of distance metric like Mahalanobis distance $d(X^i,X^j) = \\|X^i-X^j\\|_M = \\sqrt{(X^i-X^j)^TM(X^i-X^j)}$ and we try to learn the parameters of the positive semidefinite matrix $M$. Learning the Mahalanobis distance metric is equivalent to warping a rescaling of a data that replaces each point $X$ with $M^{\\frac{1}{2}}X$. You can try either scaling the data manually as suggested in previous assignment or use metric learning algorithms like [1, 2]. You can use code available online for [1].\n",
    "\n",
    "We discussed in class the effect different $k$ can have on the classification. Cross-validation can be used to decide between different values for $k$ and distance metrics.\n",
    "\n",
    "In this problem set you may use a KNN libary implentation, but will asked to try and estimate the test-error from training samples. This you will do through cross-validation. Then you will compare these estimates to the actual test error. You can create a test set by randomly selecting part of the dataset and holding them out of training and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN Deliverables\n",
    "\n",
    "For this HW you would be required to do the following for each dataset.\n",
    "\n",
    "1. Randomly select $25\\%$ of the dataset for test data (TtD). The rest will be considered the training data (TD).\n",
    "2. Randomly select a subset of $d\\%$ from the TD where $d = \\{50, 75, 100\\}$.\n",
    "3. Using each of the TD, do F-fold cross-validation (CV) on each of the generated TD. \n",
    "   * Use cross validation to decide the best K.\n",
    "   * You are expected to do 2-fold, 5-fold and Leave-one-out.\n",
    "   * Report the CV errors for the best K.\n",
    "   * [10 points]\n",
    "4. Now, test the TtD with the best K and its associated TD and compare the error rate to CV error.\n",
    "   * Report which F estimated the test error the best.\n",
    "   * [5 points]\n",
    "5. Report on the stability of the results with respect to the size of the TD. [15 points]\n",
    "6. Report on your methodology for chosing the best K and distance metric. [15 points]\n",
    "7. Report on the effect on bias and variance of the classifier w.r.t. values of K. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "---\n",
    "\n",
    "An SVM is a supervised, non-probabilistic technique for binary classification. It is also referred to as a maximum margin classifier and is efficient memory-wise since it only stores a small part of the samples for decision making. Moreover it supports Kernels for reprojecting data making it easy to harness \"Kernel Power\", and extend its utility into the non-linear domain.\n",
    "\n",
    "In this problem set you may use an SVM libary implentation, but will asked to explore different kernels their coresponding parameters and comment on your methodology. The popular choices of Kernel's in SVM literature are:\n",
    "1. Linear\n",
    "2. Polynomial\n",
    "3. Radial Basis Function (gaussian)\n",
    "4. Sigmoid\n",
    "\n",
    "For good source for understanding the basics of SVMs and data pre-processing, checkout this practical guide to support vector classification [3].\n",
    "\n",
    "Some of these kernels are parametric and will require careful tuning lest the classifier be overfitted or unstable. \n",
    "You are welcome to use the built-in sklearn utilities to find the best parameters for your SVM given the datasets. You will however be expected to understand the roles of these parameters in affecting the decision boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Deliverables\n",
    "\n",
    "For this HW you would be required to do the following for each dataset.\n",
    "\n",
    "1. Randomly select $25\\%$ of the dataset for test data (TtD). The rest will be considered the training data (TD).\n",
    "2. Randomly select a subset of $d\\%$ from the TD where $d = \\{50, 75, 100\\}$.\n",
    "3. Using each of the TD, do F-fold cross-validation (CV). \n",
    "   * Use cross validation to decide the best kernel.\n",
    "   * You are expected to do 2-fold, 5-fold.\n",
    "   * Report the CV errors for the best kernal and permaters.\n",
    "   * [10 points]\n",
    "4. Now, test the TtD with the best kernal/permaters and its associated TD and compare the error rate to CV error.\n",
    "   * Report which F estimated the test error the best.\n",
    "   * [5 points]\n",
    "5. Report on the stability of the results with respect to the size of the TD. [15 points]\n",
    "6. Report on your methodology for chosing the best kernal and permaters. [15 points]\n",
    "7. Report on the effect on bias and variance of the SVM classifier conditional on parameter values for the RBF kernel. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "---\n",
    "### Deliverables\n",
    "\n",
    "For this problem set, the two machine learning methods you are investigating are quite different. One is non-parametric memory based method, the other is very much the opposite. Using equivalent training and test sets and cross validation methods, compare the two approaches, KNN vs. SVM, from several aspects, including dataset classification accuracy, training time & testing time performance, and fitted model size in memory. Report on this comparison, justifying your conclusions using your operations as well as your understanding of the underlying implementation of both ML algorithms. [20 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "---\n",
    "Note that you may want try your classifier on smaller sets, such as the previous wine dataset, while developing to reduce training time between debugging, or provide sanity checks in prediction accuracy against naive bayes.\n",
    "\n",
    "| Dataset | Description | Data |\n",
    "|---|---|---|\n",
    "| MNIST | [[Description]](https://www.kaggle.com/c/digit-recognizer) | [[Data]](https://www.kaggle.com/c/digit-recognizer/data) |\n",
    "| Covertype | [[Description]](https://archive.ics.uci.edu/ml/datasets/Covertype) | [[Data]](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Submit\n",
    "\n",
    "---\n",
    "Submit your PDF report to Gradescope, and then same PDF with code in a zip file to Tritoned, as Gradescope does not take code submission. You can use any programming language, but we will prefer `Python` or `Matlab`. The code should be easily runnable. Please include a README.md file that describes how the TAs are to run your code. Note, you do not need to include the original data sets in the zip file (the is also a Blackboard upload limit prevents this), however please make it easy for the TAs to run you code, such that pasting the extracted course dataset folders into the root project zip folder would work, e.g something like this:\n",
    "\n",
    "```\n",
    "$ unzip Doe-John-HW2.zip\n",
    "$ unzip wine.zip -d Doe-John-HW2/wine\n",
    "$ unzip MNIST.zip -d Doe-John-HW2/MNIST \n",
    "$ tree Doe-John-HW2\n",
    "Doe-John-HW2\n",
    "|-- HW2.ipynb\n",
    "|-- HW2.pdf\n",
    "|-- MNIST\n",
    "|   |-- test.csv\n",
    "|   |-- train.csv\n",
    "|-- my_code\n",
    "|   |-- foo.py\n",
    "|   |-- bar\n",
    "|   |-- ...\n",
    "|-- README\n",
    "|-- wine\n",
    "    |-- wine.data\n",
    "\n",
    "6 directories, 7 files\n",
    "$ cat Doe-John-HW2/README\n",
    "To run my code, open HW2.ipynb and ...\n",
    "```\n",
    "\n",
    "Additionally:\n",
    "\n",
    "* Training **will** take time for the SVMs, if you plan on doing all of the parts please discount enough time for the implementation and execution.\n",
    "* You must submit your own code to get credit. No code submitted means a zero in the assignment.\n",
    "* If you are using a library from somewhere else, please mention it here as well. We hope you use Piazza for this so more people benefit.\n",
    "* You are encouraged to discuss and help others with anything short of giving them your code. There are many references on-line, especially with MNIST. However, you MAY NOT use code from the Internet.\n",
    "  * If you are working with a partner please make sure that **both** of your names are on the final PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "[1] Kilian Q. Weinberger and Lawrence K. Saul *Distance Metric Learning for Large Margin Nearest Neighbor Classification*. JMLR, 2009.  \n",
    "[2] Eric P. Xing, Andrew Y. Ng, Michael I. Jordan and Stuart Russell *Distance metric learning, with application to clustering with side-information*. NIPS, 2002.  \n",
    "[3] Hsu, Chih-Wei, Chih-Chung Chang, and Chih-Jen Lin. *A practical guide to support vector classification*. 2003 [pdf](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
